{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3cf7e91",
   "metadata": {},
   "source": [
    "# Claude ansatz torch dataset erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692b33c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "\n",
    "class H5PatchDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset für H5-Bilder mit überlappenden Patches.\n",
    "    \n",
    "    Args:\n",
    "        bild_path: Pfad zur H5-Datei mit dem Bild\n",
    "        maske_path: Pfad zur H5-Datei mit der Maske\n",
    "        patch_size: Größe der quadratischen Patches (z.B. 256)\n",
    "        overlap: Überlappung in Pixeln (z.B. 32)\n",
    "        transform: Optionale Transformationen (werden auf beide angewendet)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bild_path, maske_path, patch_size=256, overlap=32, transform=None):\n",
    "        self.bild_path = bild_path\n",
    "        self.maske_path = maske_path\n",
    "        self.patch_size = patch_size\n",
    "        self.overlap = overlap\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Lade die Daten\n",
    "        with h5py.File(bild_path, 'r') as f:\n",
    "            self.bild = np.array(f[\"Image\"])[:, :]\n",
    "        \n",
    "        with h5py.File(maske_path, 'r') as f:\n",
    "            self.maske = np.array(f[\"exported_data\"])[:, :]\n",
    "        \n",
    "        # Berechne Patch-Positionen\n",
    "        self.patch_positions = self._calculate_patch_positions()\n",
    "        \n",
    "    def _calculate_patch_positions(self):\n",
    "        \"\"\"Berechnet alle Patch-Positionen mit Überlappung.\"\"\"\n",
    "        h, w = self.bild.shape\n",
    "        stride = self.patch_size - self.overlap\n",
    "        \n",
    "        positions = []\n",
    "        for y in range(0, h - self.patch_size + 1, stride):\n",
    "            for x in range(0, w - self.patch_size + 1, stride):\n",
    "                positions.append((y, x))\n",
    "        \n",
    "        # Füge Rand-Patches hinzu, falls nötig\n",
    "        if (h - self.patch_size) % stride != 0:\n",
    "            for x in range(0, w - self.patch_size + 1, stride):\n",
    "                positions.append((h - self.patch_size, x))\n",
    "        \n",
    "        if (w - self.patch_size) % stride != 0:\n",
    "            for y in range(0, h - self.patch_size + 1, stride):\n",
    "                positions.append((y, w - self.patch_size))\n",
    "        \n",
    "        # Ecke unten rechts\n",
    "        if (h - self.patch_size) % stride != 0 and (w - self.patch_size) % stride != 0:\n",
    "            positions.append((h - self.patch_size, w - self.patch_size))\n",
    "        \n",
    "        return positions\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patch_positions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y, x = self.patch_positions[idx]\n",
    "        \n",
    "        # Extrahiere Patches\n",
    "        bild_patch = self.bild[y:y+self.patch_size, x:x+self.patch_size]\n",
    "        maske_patch = self.maske[y:y+self.patch_size, x:x+self.patch_size, :]\n",
    "        \n",
    "        # Konvertiere zu Tensoren\n",
    "        bild_tensor = torch.from_numpy(bild_patch).float().unsqueeze(0)  # (1, H, W)\n",
    "        maske_tensor = torch.from_numpy(maske_patch).float().permute(2, 0, 1)  # (3, H, W)\n",
    "        \n",
    "        # Wende Transformationen an (beide werden gleich transformiert)\n",
    "        if self.transform:\n",
    "            bild_tensor, maske_tensor = self.transform(bild_tensor, maske_tensor)\n",
    "        \n",
    "        return {\n",
    "            'image': bild_tensor,\n",
    "            'mask': maske_tensor,\n",
    "            'position': (y, x),  # Für Rekonstruktion\n",
    "            'index': idx\n",
    "        }\n",
    "    \n",
    "    def get_original_shape(self):\n",
    "        \"\"\"Gibt die ursprüngliche Bildgröße zurück.\"\"\"\n",
    "        return self.bild.shape\n",
    "\n",
    "\n",
    "class SyncedTransform:\n",
    "    \"\"\"\n",
    "    Transformationen, die synchron auf Bild und Maske angewendet werden.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rotation_range=15, flip_prob=0.5, brightness=0.2, contrast=0.2):\n",
    "        self.rotation_range = rotation_range\n",
    "        self.flip_prob = flip_prob\n",
    "        self.brightness = brightness\n",
    "        self.contrast = contrast\n",
    "    \n",
    "    def __call__(self, image, mask):\n",
    "        # Zufällige Rotation\n",
    "        if self.rotation_range > 0:\n",
    "            angle = torch.rand(1).item() * 2 * self.rotation_range - self.rotation_range\n",
    "            image = TF.rotate(image, angle, interpolation=TF.InterpolationMode.BILINEAR)\n",
    "            mask = TF.rotate(mask, angle, interpolation=TF.InterpolationMode.NEAREST)\n",
    "        \n",
    "        # Zufälliges horizontales Flip\n",
    "        if torch.rand(1).item() < self.flip_prob:\n",
    "            image = TF.hflip(image)\n",
    "            mask = TF.hflip(mask)\n",
    "        \n",
    "        # Zufälliges vertikales Flip\n",
    "        if torch.rand(1).item() < self.flip_prob:\n",
    "            image = TF.vflip(image)\n",
    "            mask = TF.vflip(mask)\n",
    "        \n",
    "        # Farbanpassungen nur auf Bild (nicht auf Maske)\n",
    "        if self.brightness > 0:\n",
    "            factor = 1 + (torch.rand(1).item() * 2 - 1) * self.brightness\n",
    "            image = TF.adjust_brightness(image, factor)\n",
    "        \n",
    "        if self.contrast > 0:\n",
    "            factor = 1 + (torch.rand(1).item() * 2 - 1) * self.contrast\n",
    "            image = TF.adjust_contrast(image, factor)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "\n",
    "def reconstruct_from_patches(predictions, positions, original_shape, patch_size, overlap):\n",
    "    \"\"\"\n",
    "    Rekonstruiert das vollständige Bild aus überlappenden Patches.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Liste von Patch-Predictions [(C, H, W), ...]\n",
    "        positions: Liste von (y, x) Positionen\n",
    "        original_shape: (H, W) des Originalbilds\n",
    "        patch_size: Größe der Patches\n",
    "        overlap: Überlappung in Pixeln\n",
    "    \n",
    "    Returns:\n",
    "        Rekonstruiertes Bild (C, H, W)\n",
    "    \"\"\"\n",
    "    h, w = original_shape\n",
    "    c = predictions[0].shape[0]\n",
    "    \n",
    "    # Erstelle Output-Array und Gewichtungs-Array\n",
    "    output = np.zeros((c, h, w), dtype=np.float32)\n",
    "    weights = np.zeros((h, w), dtype=np.float32)\n",
    "    \n",
    "    # Erstelle Gewichtungsmaske für weiche Übergänge\n",
    "    weight_mask = create_weight_mask(patch_size, overlap)\n",
    "    \n",
    "    for pred, (y, x) in zip(predictions, positions):\n",
    "        pred_np = pred.cpu().numpy() if torch.is_tensor(pred) else pred\n",
    "        output[:, y:y+patch_size, x:x+patch_size] += pred_np * weight_mask\n",
    "        weights[y:y+patch_size, x:x+patch_size] += weight_mask\n",
    "    \n",
    "    # Normalisiere durch Gewichte\n",
    "    output = output / (weights + 1e-8)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def create_weight_mask(patch_size, overlap):\n",
    "    \"\"\"Erstellt eine Gewichtungsmaske für weiche Übergänge.\"\"\"\n",
    "    mask = np.ones((patch_size, patch_size), dtype=np.float32)\n",
    "    \n",
    "    if overlap > 0:\n",
    "        fade = np.linspace(0, 1, overlap)\n",
    "        # Oben\n",
    "        mask[:overlap, :] *= fade[:, np.newaxis]\n",
    "        # Unten\n",
    "        mask[-overlap:, :] *= fade[::-1, np.newaxis]\n",
    "        # Links\n",
    "        mask[:, :overlap] *= fade[np.newaxis, :]\n",
    "        # Rechts\n",
    "        mask[:, -overlap:] *= fade[::-1][np.newaxis, :]\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3117c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Größe: 1044 Patches\n",
      "Original Bildgröße: (7956, 6488)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cwd = Path.cwd()\n",
    "path = Path('PE-2024-01126-M_00_s0021_PM_Complete_Transmittance_Stitched_Flat_v004.h5')\n",
    "fullpath = Path.cwd().parent/f'data/{path}'\n",
    "Maskpath = Path('PE-2024-01126-M_00_s0021_PM_Complete_Transmittance_Stitched_Flat_v004-Image_Probabilities.h5')\n",
    "Maskfullpath = Path.cwd().parent/f'data/{Maskpath}'\n",
    "# Dataset erstellen\n",
    "transform = SyncedTransform(rotation_range=15, flip_prob=0.5)\n",
    "dataset = H5PatchDataset(\n",
    "\tbild_path=fullpath,\n",
    "\tmaske_path=Maskfullpath,\n",
    "\tpatch_size=256,\n",
    "\toverlap=32,\n",
    "\ttransform=transform\n",
    ")\n",
    "\n",
    "print(f\"Dataset Größe: {len(dataset)} Patches\")\n",
    "print(f\"Original Bildgröße: {dataset.get_original_shape()}\")\n",
    "\n",
    "# DataLoader erstellen\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "\n",
    "# Beispiel: Ein Batch laden\n",
    "batch = next(iter(dataloader))\n",
    "print(f\"Batch Image Shape: {batch['image'].shape}\")\n",
    "print(f\"Batch Mask Shape: {batch['mask'].shape}\")\n",
    "\n",
    "# Beispiel: Rekonstruktion (im Inference-Modus ohne Transform)\n",
    "test_dataset = H5PatchDataset(\n",
    "\tbild_path=\"path/to/image.h5\",\n",
    "\tmaske_path=\"path/to/mask.h5\",\n",
    "\tpatch_size=256,\n",
    "\toverlap=32,\n",
    "\ttransform=None  # Keine Augmentation für Inferenz\n",
    ")\n",
    "\n",
    "# Sammle alle Predictions (hier als Beispiel die Masken selbst)\n",
    "predictions = []\n",
    "positions = []\n",
    "for sample in test_dataset:\n",
    "\tpredictions.append(sample['mask'])\n",
    "\tpositions.append(sample['position'])\n",
    "\n",
    "# Rekonstruiere\n",
    "reconstructed = reconstruct_from_patches(\n",
    "\tpredictions, \n",
    "\tpositions, \n",
    "\ttest_dataset.get_original_shape(),\n",
    "\t256, \n",
    "\t32\n",
    ")\n",
    "print(f\"Rekonstruiertes Bild Shape: {reconstructed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c4fcd0",
   "metadata": {},
   "source": [
    "## CNN aus meiner deep learning übung\n",
    "Ist nicht sehr sinnvoll für unsere anwendung, da hier nur 10 outputs exestieren.... muss also angepasst werden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a62ee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=0, dilation=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=5, stride=1, padding=0, dilation=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0, dilation=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=0, dilation=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32*4*4, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 32*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "\n",
    "    # Define the train_net function.\n",
    "    def train_net(self, criterion, optimizer, trainloader, epochs, _net=\"CNN\"):\n",
    "        log_interval = 10\n",
    "        for epoch in range(epochs):\n",
    "            for batch_idx, (data, target) in enumerate(trainloader):\n",
    "                data, target = Variable(data), Variable(target)  # data, target = data.to(device), target.to(device)\n",
    "                if _net == \"MLP\":\n",
    "                    data = data.view(-1, 3 * 32 * 32)\n",
    "                optimizer.zero_grad()\n",
    "                net_out = self(data)\n",
    "                loss = criterion(net_out, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if batch_idx % log_interval == 0:\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data),\n",
    "                                                                                   len(trainloader.dataset),\n",
    "                                                                                   100. * batch_idx / len(trainloader),\n",
    "                                                                                   loss.data.item()))\n",
    "\n",
    "    # Define the test_net function.\n",
    "    def test_net(self, criterion, testloader, _net=\"CNN\"):\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for i_batch, (data, target) in enumerate(testloader):\n",
    "            data, target = Variable(data), Variable(target)  # data, target = data.to(device), target.to(device)\n",
    "            if _net == \"MLP\":\n",
    "                data = data.view(-1, 3 * 32 * 32)\n",
    "            net_out = self(data)\n",
    "            test_loss += criterion(net_out, target).data.item()  # sum up batch loss\n",
    "            pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "            batch_labels = pred.eq(target.data)\n",
    "            correct += batch_labels.sum()\n",
    "        test_loss /= len(testloader.dataset)\n",
    "        acc = 100. * float(correct) / len(testloader.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(test_loss, correct,\n",
    "                                                                                     len(testloader.dataset), acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca079da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model.\n",
    "model = Net()\n",
    "model_type = \"CNN\"\n",
    "\n",
    "# Create the optimizer and the (loss-)criterion.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Train and save.\n",
    "model.train()\n",
    "model.train_net(criterion, optimizer, trainloader, 2, _net=model_type)\n",
    "torch.save(model.state_dict(), f\"data/net_{model_type}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PPM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
